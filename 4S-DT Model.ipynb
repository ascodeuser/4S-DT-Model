{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e609238",
   "metadata": {},
   "source": [
    
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f02eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob as gb\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from skimage.io import imread, imshow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Add, Dense, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from mlxtend.plotting import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7416d29a",
   "metadata": {},
   "source": [
    "## Loading the decomposed dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652aae01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the data path\n",
    "trainpath= ('...../training_set/')\n",
    "\n",
    "img_height=224\n",
    "img_width=224\n",
    "batch_size=64\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "    validation_split=0.2) # set validation split\n",
    "\n",
    "print(\"The data is being split into training and validation set\")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    trainpath,# This is the target directory\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training') # set as training data\n",
    "print(\"----------------------------------------------------------------\")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    trainpath, # same directory as training data\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation') # set as validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634a0654",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_generator.class_indices\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57ed47f",
   "metadata": {},
   "source": [
    "## Loading the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d3a1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    testpath,# This is the target directory\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=1,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False,  #because you need to yield the images in ‚Äúorder‚Äù, to predict the outputs and match them with their unique ids or filenames.\n",
    "    seed=42) # set as training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9e69a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test , y_test = [] , []\n",
    "for i in range(test_generator.n//1):\n",
    "    a , b = test_generator.next()\n",
    "    x_test.extend(a) \n",
    "    y_test.extend(b)\n",
    "y_test= np.array(y_test)\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f17e5d5",
   "metadata": {},
   "source": [
    "## Fine-tuning the learned parameters from the pretext training model into a new downstream task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0370896d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the pretext training model\n",
    "pretext_training =load_model('........../pretext_CXR.hdf5')\n",
    "pretext_training.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735c6146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the classification layer from the pretext model\n",
    "model = Model(inputs=pretext_training.input,   outputs = pretext_training.get_layer('dense').output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4134ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the new classification output layer corresponding to the new downstream task\n",
    "new_prediction =Dense(len(train_label), activation='softmax', name=\"new_task\")(model.output)\n",
    "\n",
    "# Building the final 4S_DT model and visualize it\n",
    "S4_TD_model = Model(inputs=model.input, outputs=new_prediction)\n",
    "\n",
    "# Freeze all layers initially\n",
    "for layer in S4_TD_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Total number of layers\n",
    "total_layers = len(S4_TD_model.layers)\n",
    "\n",
    "# Number of layers to progressively unfreeze\n",
    "# 4S-DT model used the 4 last layers\n",
    "N = 4\n",
    "\n",
    "S4_TD_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb21570",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95f9256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Progressive fine-tuning: from last layer up to last N layers\n",
    "for i in range(1, N + 1):\n",
    "    print(f\"\\n--- Training with last {i} layer(s) unfrozen ---\")\n",
    "\n",
    "    # Unfreeze the last i layers\n",
    "    for layer in S4_TD_model.layers[-i:]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    # Compile the model again after changing trainable layers\n",
    "    S4_TD_model.compile(\n",
    "        optimizer=SGD(learning_rate=0.001, momentum=0.9),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    # Define callbacks\n",
    "    layer_name = f\"last_{i}_layers\"\n",
    "    callbacks = training_opt(layer_name)\n",
    "\n",
    "    # Train the model\n",
    "    \n",
    "    history = S4_TD_model.fit(\n",
    "        train_generator,\n",
    "        validation_data=validation_generator,\n",
    "        epochs=50,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "\n",
    "    # Load best weights and evaluate\n",
    "    best_weights_path = f'.............../CXR/results/{layer_name}.hdf5'\n",
    "    S4_TD_model.load_weights(best_weights_path)\n",
    "\n",
    "    # Load best weights\n",
    "    best_weights_path = os.path.join('....../CXR/results/', f\"{layer_name}.hdf5\")\n",
    "    S4_TD_model.load_weights(best_weights_path)\n",
    "\n",
    "    # Predict on test set\n",
    "    y_pred_probs = S4_TD_model.predict(x_test)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "    # error correction step\n",
    "    k_value = 5        # Define the k value used to decompose the training set\n",
    "    corrected_pred = np.copy(y_pred)\n",
    "    for idx in range(len(corrected_pred)):\n",
    "        corrected_pred[idx] = corrected_pred[idx] // k_value\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    fig, ax = plot_confusion_matrix(conf_mat=cm, figsize=(5, 5))\n",
    "    plt.title(f\"Confusion Matrix: {layer_name}\")\n",
    "    plt.show()\n",
    "\n",
    "    # Classification report\n",
    "    print(f\"üßæ Classification Report (Layer: {layer_name}):\")\n",
    "    print(classification_report(y_true, corrected_pred, target_names=test_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
